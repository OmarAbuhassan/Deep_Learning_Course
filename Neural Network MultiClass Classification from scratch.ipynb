{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network: MultiClass Classification from scratch\n",
    "We will modify the architecture that developed in part A to use it for multiclass classification on **Statlog (Vehicle Silhouettes)** Data Set.\n",
    "\n",
    "More details on the dataset can be found at: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "from fast_ml.model_development import train_valid_test_split\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPACTNESS</th>\n",
       "      <th>CIRCULARITY</th>\n",
       "      <th>DISTANCE CIRCULARITY</th>\n",
       "      <th>RADIUS RATIO</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>Statlog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95</td>\n",
       "      <td>48</td>\n",
       "      <td>83</td>\n",
       "      <td>178</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>162</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>159</td>\n",
       "      <td>176</td>\n",
       "      <td>379</td>\n",
       "      <td>184</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>187</td>\n",
       "      <td>197</td>\n",
       "      <td>van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>91</td>\n",
       "      <td>41</td>\n",
       "      <td>84</td>\n",
       "      <td>141</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>149</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>170</td>\n",
       "      <td>330</td>\n",
       "      <td>158</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>189</td>\n",
       "      <td>199</td>\n",
       "      <td>van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>104</td>\n",
       "      <td>50</td>\n",
       "      <td>106</td>\n",
       "      <td>209</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>207</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>158</td>\n",
       "      <td>223</td>\n",
       "      <td>635</td>\n",
       "      <td>220</td>\n",
       "      <td>73</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>188</td>\n",
       "      <td>196</td>\n",
       "      <td>saab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>93</td>\n",
       "      <td>41</td>\n",
       "      <td>82</td>\n",
       "      <td>159</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>144</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>160</td>\n",
       "      <td>309</td>\n",
       "      <td>127</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>199</td>\n",
       "      <td>207</td>\n",
       "      <td>van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>44</td>\n",
       "      <td>70</td>\n",
       "      <td>205</td>\n",
       "      <td>103</td>\n",
       "      <td>52</td>\n",
       "      <td>149</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>144</td>\n",
       "      <td>241</td>\n",
       "      <td>325</td>\n",
       "      <td>188</td>\n",
       "      <td>127</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>180</td>\n",
       "      <td>183</td>\n",
       "      <td>bus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COMPACTNESS  CIRCULARITY  DISTANCE CIRCULARITY  RADIUS RATIO    4   5    6  \\\n",
       "0           95           48                    83           178   72  10  162   \n",
       "1           91           41                    84           141   57   9  149   \n",
       "2          104           50                   106           209   66  10  207   \n",
       "3           93           41                    82           159   63   9  144   \n",
       "4           85           44                    70           205  103  52  149   \n",
       "\n",
       "    7   8    9   10   11   12   13  14  15   16   17 Statlog  \n",
       "0  42  20  159  176  379  184   70   6  16  187  197     van  \n",
       "1  45  19  143  170  330  158   72   9  14  189  199     van  \n",
       "2  32  23  158  223  635  220   73  14   9  188  196    saab  \n",
       "3  46  19  143  160  309  127   63   6  10  199  207     van  \n",
       "4  45  19  144  241  325  188  127   9  11  180  183     bus  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Vehicle.csv\", header=None)\n",
    "df.rename(columns = {0:\"COMPACTNESS\", 1:\"CIRCULARITY\", 2:\"DISTANCE CIRCULARITY\", 3:\"RADIUS RATIO\", 18:\"Statlog\"},inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bus     218\n",
       "saab    217\n",
       "opel    212\n",
       "van     199\n",
       "Name: Statlog, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Statlog.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating instance of one-hot-encoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "# passing bridge-types-cat column (label encoded values of bridge_types)\n",
    "enc_df = pd.DataFrame(enc.fit_transform(df[['Statlog']]).toarray())\n",
    "\n",
    "Y = enc_df.to_numpy()\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 95,  48,  83, ...,  16, 187, 197],\n",
       "        [ 91,  41,  84, ...,  14, 189, 199],\n",
       "        [104,  50, 106, ...,   9, 188, 196],\n",
       "        ...,\n",
       "        [106,  54, 101, ...,   4, 187, 201],\n",
       "        [ 86,  36,  78, ...,  25, 190, 195],\n",
       "        [ 85,  36,  66, ...,  18, 186, 190]], dtype=int64),\n",
       " (846, 18))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['Statlog'], axis='columns')\n",
    "X = X.to_numpy()\n",
    "X , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, X_rem, train_labels, y_rem = train_test_split(X,Y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data, test_data, valid_labels, test_labels = train_test_split(X_rem,y_rem, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 85,  43,  69, ...,   4, 181, 184],\n",
       "       [ 88,  39,  76, ...,   6, 201, 209],\n",
       "       [ 85,  47,  75, ...,   7, 182, 191],\n",
       "       ...,\n",
       "       [101,  53, 108, ...,  24, 190, 201],\n",
       "       [ 89,  38,  74, ...,   7, 189, 193],\n",
       "       [ 89,  36,  72, ...,   1, 187, 192]], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "valid_data = scaler.transform(valid_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx = train_data.T\n",
    "trainy = train_labels.T\n",
    "validx = valid_data.T\n",
    "validy = valid_labels.T\n",
    "testx = test_data.T\n",
    "testy = test_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18, 592), (4, 592), (18, 127), (4, 127), (18, 127), (4, 127))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape, trainy.shape, validx.shape, validy.shape, testx.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = trainx\n",
    "Y = trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training samples: 592\n",
      "Number of features per sample: 18\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = shape_X[1]  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('No. of training samples: ' + str(m))\n",
    "print ('Number of features per sample: ' + str(shape_X[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_architecture(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    n_x = shape_X[0] # size of input layer\n",
    "    n_h = 10 \n",
    "    n_y = shape_Y[0] # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### Update THE CODE HERE ###\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(vector):\n",
    "    e = np.exp(vector)\n",
    "    return e / e.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters.get('W1')\n",
    "    b1 = parameters.get('b1')\n",
    "    W2 = parameters.get('W2')\n",
    "    b2 = parameters.get('b2')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### \n",
    "    Z1 = np.dot(W1, X) + b1 \n",
    "    A1 = np.tanh(Z1)\n",
    "#     A1 = relu(Z1)\n",
    "#     A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "   \n",
    "    assert(A2.shape == (4, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "       \n",
    "    Returns:\n",
    "    loss -- cross-entropy loss given equation (7)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    ### START CODE HERE ###\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
    "    loss = - np.sum(logprobs)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    loss = float(np.squeeze(loss))\n",
    "\n",
    "    assert(isinstance(loss, float))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data\n",
    "    Y -- \"true\" labels\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters.get('W1')\n",
    "    W2 = parameters.get('W2')\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### \n",
    "    A1 = cache.get('A1')\n",
    "    A2 = cache.get('A2')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    learning_rate -- The learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters.get('W1')\n",
    "    b1 = parameters.get('b1')\n",
    "    W2 = parameters.get('W2')\n",
    "    b2 = parameters.get('b2')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### \n",
    "    dW1 = grads.get('dW1')\n",
    "    db1 = grads.get('db1')\n",
    "    dW2 = grads.get('dW2')\n",
    "    db2 = grads.get('db2')\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_loss=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset\n",
    "    Y -- labels \n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    learning_rate -- The learning rate\n",
    "    print_loss -- if True, print the loss every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = model_architecture(X, Y)[0]\n",
    "    n_y = model_architecture(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
    "        loss = compute_loss(A2, Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backprop(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters =  update(parameters, grads, learning_rate = 0.01)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the loss every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data \n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### \n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    \n",
    "    predictions = np.argmax(A2, axis = 0)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 1, num_iterations = 10000, learning_rate = 0.1, print_loss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49324324324324326\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(np.argmax(Y.T, axis = 1), predictions.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5039370078740157\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions_validation = predict(parameters, validx)\n",
    "print(accuracy_score(np.argmax(validy.T, axis = 1), predictions_validation.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3700787401574803\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(np.argmax(testy.T, axis = 1), predictions_test.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
