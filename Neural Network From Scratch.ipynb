{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0A4Q6DBB3FVY"
   },
   "source": [
    "# Neural Network Implementation-From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZ7Bd9ZsoXCR"
   },
   "source": [
    "## A. Neural Network: Binary Classification \n",
    "\n",
    "In this assignment, we will learn to build a fully-connected neural network with standard architecture, i.e., only one hidden layer. We will use the `Breast cancer wisconsin (diagnostic) dataset` available in `https://scikit-learn.org/stable/datasets/index.html`. It has a total of 569 sample with two classes (Malignant and Benign). Each sample has 30 real-valued features. \n",
    "\n",
    "**This assignment will help you understand and implement:**\n",
    "- A neural network for binary classification consisting of one hidden layer with non-linear activation. You will implement ideas like forward propogation, computing the loss, bagward propogation, and parameter(weights) update. Using the trained network, you will learn to predict a class given the features of a sample.\n",
    "\n",
    "**Note:** There are multiple conventions for coding neural networks. We will follow the conventions suggested by Andrew Ng: https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "sqaGHZSJoXCU"
   },
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGkbrWCS3FVm"
   },
   "source": [
    "## 1. Loading the dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LENi5Q9Z3FVn"
   },
   "outputs": [],
   "source": [
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, y, test_size =0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "LkzSuMSr3FVo"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(train_data)\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((455, 30), (1, 455), (30, 114), (1, 114))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, trainy.shape, testx.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "rpzj7FmV3FVq"
   },
   "outputs": [],
   "source": [
    "trainx = train_data.T\n",
    "trainy = train_labels.reshape(-1,1).T\n",
    "\n",
    "testx = test_data.T\n",
    "testy =test_labels.reshape(-1,1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "d78k8Prk3FVs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 455), (1, 455), (30, 114), (1, 114))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainx.shape, trainy.shape, testx.shape, testy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0dIZ3-k23FVu"
   },
   "outputs": [],
   "source": [
    "X=trainx\n",
    "Y=trainy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DFAQt6HYoXCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of training samples: 455\n",
      "Number of features per sample: 30\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = shape_X[1]  # training set size\n",
    "### END CODE HERE ###\n",
    "\n",
    "print ('No. of training samples: ' + str(m))\n",
    "print ('Number of features per sample: ' + str(shape_X[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2AFzm-hoXCv"
   },
   "source": [
    "## 2 - Neural Network model\n",
    "\n",
    "We will train a Neural Network with a single hidden layer.\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost (loss) $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Important**: Building the NN will involve the following:\n",
    "\n",
    "    1. Specify the network structure in terms of the number of input units, number of neurons in the hidden units, ...\n",
    "    2. Initialize the parameters of the model\n",
    "    3. Loop a number of iterations:\n",
    "        - Forward propagation\n",
    "        - Compute loss and the overall loss\n",
    "        - Backward propagation\n",
    "        - Update the parameters (gradient descent)\n",
    "\n",
    "In order to make the code modular, we can implement each of the step as a function and them combine them together to build the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dC3UrB-AoXCw"
   },
   "source": [
    "### 2.1 - Specify the network structure \n",
    "    - n_x: input layer size\n",
    "    - n_h: #neurons in  hidden layer (hard code a value, say 10) \n",
    "    - n_y: the size of the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "mOsdX_bPoXCx"
   },
   "outputs": [],
   "source": [
    "def model_architecture(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### \n",
    "    n_x = shape_X[0] # size of input layer\n",
    "    n_h = 10 \n",
    "    n_y = 1 # size of output layer\n",
    "    ### END CODE HERE ###\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4-FAtl-oXC3"
   },
   "source": [
    "### 2.2 - Initialize the parameters of the model\n",
    "\n",
    "- Initialize the weights matrices with random values. \n",
    "    - Use: `np.random.randn(a,b) * 0.01` to randomly initialize a matrix of shape (a,b).\n",
    "- Initialize the bias vectors as zeros. \n",
    "    - Use: `np.zeros((a,b))` to initialize a matrix of shape (a,b) with zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "XV3W6uLvoXC3"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2)\n",
    "\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBRbnqYkoXC8"
   },
   "source": [
    "### 2.3 - The Loop ####\n",
    "\n",
    "**Instructions**:\n",
    "- Look above at the mathematical representation of your classifier.\n",
    "- Define the function `sigmoid()`.\n",
    "- You can use the function `np.tanh()`. It is part of the numpy library.\n",
    "- The steps you have to implement are:\n",
    "    1. Retrieve each parameter from the dictionary \"parameters\" (which is the output of `initialize_parameters()`) by using `parameters[\"..\"]`.\n",
    "    2. Implement Forward Propagation. Compute $Z^{[1]}, A^{[1]}, Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set).\n",
    "- Values needed in the backpropagation are stored in \"`cache`\". The `cache` will be given as an input to the backpropagation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "-4Xif_6C3FV3"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    ### Update THE CODE HERE ###\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return np.maximum(0,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "AkJ4TKKSoXC9"
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters.get('W1')\n",
    "    b1 = parameters.get('b1')\n",
    "    W2 = parameters.get('W2')\n",
    "    b2 = parameters.get('b2')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    ### START CODE HERE ### \n",
    "    Z1 = np.dot(W1, X) + b1 \n",
    "    A1 = np.tanh(Z1)\n",
    "    A1 = relu(Z1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21VnrjU3oXDC"
   },
   "source": [
    "Now you have computed $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for every example, you can compute the loss function as follows:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{7}$$\n",
    "\n",
    "- Implement the cross-entropy loss:\n",
    "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "loss = - np.sum(logprobs)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "whxXuxq-oXDD"
   },
   "outputs": [],
   "source": [
    "def compute_loss(A2, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "       \n",
    "    Returns:\n",
    "    loss -- cross-entropy loss given equation (7)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "\n",
    "    # Compute the cross-entropy loss\n",
    "    ### START CODE HERE ###\n",
    "#     cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A))) \n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
    "    loss = - np.sum(logprobs)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    loss = float(np.squeeze(loss))\n",
    "\n",
    "    assert(isinstance(loss, float))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5puT2VVoXDH"
   },
   "source": [
    "Using the cache computed during forward propagation, we can now implement backward propagation.\n",
    "\n",
    "Backpropagation is usually the hardest (most mathematical) part. You can use the following six equations as vectorized implementation:\n",
    "\n",
    "$$dZ^{[2]} = A^{[2]} - Y \\tag{8}$$ \n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]}A^{[1]{T}} \\tag{9}$$ \n",
    "$$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[2]}, axis = 1, keepdims = True)\\tag{10}$$ \n",
    "$$dZ^{[1]} = W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]}) \\tag{11}$$ \n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]}X^{{T}} \\tag{12}$$ \n",
    "$$db^{[1]} = \\frac{1}{m} np.sum(dZ^{[1]}, axis = 1, keepdims = True)\\tag{13}$$ \n",
    "\n",
    "- $*$ denotes elementwise multiplication.\n",
    "- Notations followed:\n",
    "    - dW1 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_1 }$\n",
    "    - db1 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_1 }$\n",
    "    - dW2 = $\\frac{\\partial \\mathcal{J} }{ \\partial W_2 }$\n",
    "    - db2 = $\\frac{\\partial \\mathcal{J} }{ \\partial b_2 }$\n",
    "   \n",
    "- Tips:\n",
    "    - To compute dZ1 you'll need to compute $g^{[1]'}(Z^{[1]})$. Since $g^{[1]}(.)$ is the tanh activation function, if $a = g^{[1]}(z)$ then $g^{[1]'}(z) = 1-a^2$. So you can compute \n",
    "    $g^{[1]'}(Z^{[1]})$ using `(1 - np.power(A1, 2))`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "KHTVtf4_oXDJ"
   },
   "outputs": [],
   "source": [
    "def backprop(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data\n",
    "    Y -- \"true\" labels\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters.get('W1')\n",
    "    W2 = parameters.get('W2')\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    ### START CODE HERE ### \n",
    "    A1 = cache.get('A1')\n",
    "    A2 = cache.get('A2')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    ### START CODE HERE ### \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsDhe51IoXDO"
   },
   "source": [
    "Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2).\n",
    "\n",
    "**Gradient descent rule**: $ \\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter.\n",
    "\n",
    "**Illustration**: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley.\n",
    "\n",
    "<img src=\"images\\sgd.gif\" style=\"width:400;height:400;\"> <img src=\"images\\sgd_bad.gif\" style=\"width:400;height:400;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "xWeErPRmoXDP"
   },
   "outputs": [],
   "source": [
    "def update(parameters, grads, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    learning_rate -- The learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    ### START CODE HERE ### \n",
    "    W1 = parameters.get('W1')\n",
    "    b1 = parameters.get('b1')\n",
    "    W2 = parameters.get('W2')\n",
    "    b2 = parameters.get('b2')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    ### START CODE HERE ### \n",
    "    dW1 = grads.get('dW1')\n",
    "    db1 = grads.get('db1')\n",
    "    dW2 = grads.get('dW2')\n",
    "    db2 = grads.get('db2')\n",
    "    ## END CODE HERE ###\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    ### START CODE HERE ### \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSWq5OE4oXDT"
   },
   "source": [
    "### 2.4 - Integrate parts 2.1, 2.2 and 2.3 in NeuralNetwork() ####\n",
    "\n",
    "Build your neural network model in `NeuralNetwork()`.\n",
    "\n",
    "**Instructions**: The neural network model has to use the previous functions in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "cnSyk2auoXDU"
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(X, Y, n_h, num_iterations = 10000, learning_rate = 0.01, print_loss=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset\n",
    "    Y -- labels \n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    learning_rate -- The learning rate\n",
    "    print_loss -- if True, print the loss every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to make predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = model_architecture(X, Y)[0]\n",
    "    n_y = model_architecture(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    ### START CODE HERE ### \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        ### START CODE HERE ### \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # loss function. Inputs: \"A2, Y, parameters\". Outputs: \"loss\".\n",
    "        loss = compute_loss(A2, Y)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backprop(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters =  update(parameters, grads, learning_rate = 0.01)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Print the loss every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            print (\"loss after iteration %i: %f\" %(i, loss))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Ud31U_ZoXDY"
   },
   "source": [
    "### 2.5 Predictions\n",
    "\n",
    "Use your model to predict by building predict().\n",
    "\n",
    "**Reminder**: $ predictions = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    \n",
    "As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: ```X_new = (X > threshold)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "YPMG_zMloXDZ"
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data \n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### \n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC3OHe-9oXDd"
   },
   "source": [
    "## 3. Model Execution\n",
    "It is time to run the model and see how it performs on the dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "op5NM0gXoXDi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 1, num_iterations = 10000, learning_rate = 0.01, print_loss=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8vFWiwz3FWA"
   },
   "source": [
    "## Check the accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BUdvhN9BoXDn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989010989010989\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(Y.T, predictions.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjg3DINw3FWB"
   },
   "source": [
    "## Check the accuracy on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "2-pHP3ts3FWB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(testy.T, predictions_test.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrT2xrXeoXDz"
   },
   "source": [
    "## Run the model multiple times with different hyperparameters and write your interpretation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989010989010989\n",
      "0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "#Change the learning rate to be 0.1 \n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 1, num_iterations = 10000, learning_rate = 0.1, print_loss=False)\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(Y.T, predictions.T))\n",
    "\n",
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(testy.T, predictions_test.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6373626373626373\n",
      "0.5877192982456141\n"
     ]
    }
   ],
   "source": [
    "#Change the number of iteration to be 100 \n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 1, num_iterations = 100, learning_rate = 0.01, print_loss=False)\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(Y.T, predictions.T))\n",
    "\n",
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(testy.T, predictions_test.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72jTqgYDe1vI"
   },
   "source": [
    "#### As shown, the changing in learning rate does not have an affect. However, reducing number of iteration affected the accracy score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66OLHmwvoXD0"
   },
   "source": [
    "### What happens when you change the tanh activation for a sigmoid activation or a ReLU activation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-455ffaacaa75>:17: RuntimeWarning: divide by zero encountered in log\n",
      "  logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
      "<ipython-input-39-455ffaacaa75>:17: RuntimeWarning: invalid value encountered in multiply\n",
      "  logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1-A2),(1-Y))\n",
      "<ipython-input-36-d0972ee73031>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n",
      "<ipython-input-40-7133fc7fd122>:31: RuntimeWarning: overflow encountered in power\n",
      "  dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n",
      "<ipython-input-40-7133fc7fd122>:31: RuntimeWarning: invalid value encountered in multiply\n",
      "  dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3626373626373626\n",
      "0.41228070175438597\n"
     ]
    }
   ],
   "source": [
    "#After changing the activation func. to be ReLU\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 1, num_iterations = 10000, learning_rate = 0.1, print_loss=False)\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(Y.T, predictions.T))\n",
    "\n",
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(testy.T, predictions_test.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-d0972ee73031>:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.967032967032967\n",
      "0.9385964912280702\n"
     ]
    }
   ],
   "source": [
    "#After changing the activation func. to be Sigmoid\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = NeuralNetwork(X, Y, 1, num_iterations = 10000, learning_rate = 0.1, print_loss=False)\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, X)\n",
    "print(accuracy_score(Y.T, predictions.T))\n",
    "\n",
    "predictions_test = predict(parameters, testx)\n",
    "print(accuracy_score(testy.T, predictions_test.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, ReLU cause many warning and the accuracy is very low comparing with tanh. Yet, Sigmoid is much better than ReLU but still less than tanh.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Neural Network: MultiClass Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkgI-4O83FWC"
   },
   "source": [
    "Modify the previous architecture to model multi-class classification task. Test your architecture on the **Statlog (Vehicle Silhouettes)** Data Set ('Vehicles.csv'). Save your solution as a seperate notebook file with appropriate filename.\n",
    "\n",
    "**Note:**\n",
    "1. Perform the train/validate/test split as 70/15/15.\n",
    "2. Use Random seed as '777' wherever needed.\n",
    "3. Report appropriate measures in addition to accuracy and also plot the confusion matrix.\n",
    "\n",
    "More details on the dataset can be found at: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "i6Bo1sU2fWVM"
   },
   "outputs": [],
   "source": [
    "# write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WvoglA35oXD3"
   },
   "source": [
    "References:\n",
    "- https://www.coursera.org/learn/neural-networks-deep-learning\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/\n",
    "- http://cs231n.github.io/neural-networks-case-study/\n",
    "- https://archive.ics.uci.edu/ml/datasets.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD34rdqo3FWE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Neural Network From Scratch.ipynb",
   "provenance": []
  },
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
